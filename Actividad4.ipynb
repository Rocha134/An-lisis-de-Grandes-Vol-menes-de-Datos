{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e7452d",
   "metadata": {},
   "source": [
    "### 1. Construcción de la muestra **M**\n",
    "\n",
    "**Objetivo:** generar una muestra representativa **M** de la población **P** (datos IEEE-CIS Fraud Detection), preservando la distribución natural de las variables de caracterización:\n",
    "\n",
    "* **`isFraud`**  (0 = legítimo, 1 = fraude)  \n",
    "* **`ProductCD`**  (W, C, R, H, S)\n",
    "\n",
    "Creamos 10 estratos \\(Mi = \\{\\,\\text{isFraud}=x,\\; \\text{ProductCD}=y\\,\\}\\).  \n",
    "Para evitar sesgos:\n",
    "\n",
    "1. **Muestreo proporcional (5 %)** dentro de cada estrato → mantiene la distribución global.  \n",
    "2. **Umbral mínimo** de **150 filas** por estrato para que los grupos pequeños queden bien representados.  \n",
    "3. Si un estrato tiene < 150 registros, se selecciona **todo** el estrato (no altera la proporción global de forma significativa dada la población de 590 k).\n",
    "\n",
    "El resultado es un DataFrame **`M`** y 10 particiones **`Mi`** cacheados para uso posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2baeb918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/08 21:17:25 WARN Utils: Your hostname, Franciscos-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.100.73 instead (on interface en0)\n",
      "25/06/08 21:17:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/08 21:17:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/08 21:17:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transacciones totales en población P: 590,540\n",
      "\n",
      "Fracciones aplicadas por estrato:\n",
      "  0_C    → 0.050\n",
      "  0_H    → 0.050\n",
      "  0_R    → 0.050\n",
      "  0_S    → 0.050\n",
      "  0_W    → 0.050\n",
      "  1_C    → 0.050\n",
      "  1_H    → 0.095\n",
      "  1_R    → 0.105\n",
      "  1_S    → 0.219\n",
      "  1_W    → 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño final de la muestra M: 29,966\n",
      "   Mi=0_C     →  3082 filas\n",
      "   Mi=0_H     →  1610 filas\n",
      "   Mi=0_R     →  1785 filas\n",
      "   Mi=0_S     →   554 filas\n",
      "   Mi=0_W     →  21604 filas\n",
      "   Mi=1_C     →   409 filas\n",
      "   Mi=1_H     →   139 filas\n",
      "   Mi=1_R     →   184 filas\n",
      "   Mi=1_S     →   149 filas\n",
      "   Mi=1_W     →   450 filas\n",
      "\n",
      "Distribución isFraud en M:\n",
      "+-------+-----+\n",
      "|isFraud|count|\n",
      "+-------+-----+\n",
      "|      1| 1331|\n",
      "|      0|28635|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------- Sección 1 • Código: Construcción de la muestra M -------------\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pathlib import Path\n",
    "\n",
    "# 1️⃣  Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Actividad4_MuestraM\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2️⃣  Rutas de los CSV\n",
    "DATA_DIR = Path(\"/Users/rocha/Desktop/An-lisis-de-Grandes-Vol-menes-de-Datos\")\n",
    "path_tx  = DATA_DIR / \"train_transaction.csv\"\n",
    "path_id  = DATA_DIR / \"train_identity.csv\"\n",
    "\n",
    "# 3️⃣  Carga y unión\n",
    "tx_df = spark.read.csv(str(path_tx), header=True, inferSchema=True)\n",
    "id_df = spark.read.csv(str(path_id), header=True, inferSchema=True)\n",
    "\n",
    "full_df = tx_df.join(id_df, on=\"TransactionID\", how=\"left\").cache()\n",
    "print(f\"Transacciones totales en población P: {full_df.count():,}\")\n",
    "\n",
    "# 4️⃣  Generar clave de estrato  (isFraud_ProductCD)\n",
    "full_df = full_df.withColumn(\n",
    "    \"stratum\", F.concat_ws(\"_\", F.col(\"isFraud\").cast(\"string\"), F.col(\"ProductCD\"))\n",
    ")\n",
    "\n",
    "# 5️⃣  Conteo por estrato\n",
    "stratum_counts = (\n",
    "    full_df.groupBy(\"stratum\").count()\n",
    "           .orderBy(\"stratum\")\n",
    "           .collect()\n",
    ")\n",
    "\n",
    "# 6️⃣  Calcular fracciones: 5 % o lo necesario para ≥150 filas\n",
    "SAMPLE_BASE = 0.05            # 5 %\n",
    "MIN_PER_STRATUM = 150\n",
    "\n",
    "fractions = {}\n",
    "for row in stratum_counts:\n",
    "    s = row[\"stratum\"]\n",
    "    n = row[\"count\"]\n",
    "    frac = max(SAMPLE_BASE, MIN_PER_STRATUM / n)\n",
    "    fractions[s] = min(frac, 1.0)   # nunca >1\n",
    "\n",
    "print(\"\\nFracciones aplicadas por estrato:\")\n",
    "for k, v in fractions.items():\n",
    "    print(f\"  {k:6s} → {v:.3f}\")\n",
    "\n",
    "# 7️⃣  Muestreo estratificado\n",
    "M_df = (\n",
    "    full_df.sampleBy(\"stratum\", fractions=fractions, seed=42)\n",
    "           .cache()\n",
    "           .drop(\"stratum\")\n",
    ")\n",
    "\n",
    "print(f\"\\nTamaño final de la muestra M: {M_df.count():,}\")\n",
    "\n",
    "# 8️⃣  Crear y almacenar cada partición Mi en un diccionario\n",
    "partitions = {}\n",
    "for row in stratum_counts:\n",
    "    key = row[\"stratum\"]\n",
    "    label, prod = key.split(\"_\")\n",
    "    part_df = M_df.filter((F.col(\"isFraud\") == int(label)) & (F.col(\"ProductCD\") == prod)).cache()\n",
    "    partitions[key] = part_df\n",
    "    print(f\"   Mi={key:6s}  →  {part_df.count():4d} filas\")\n",
    "\n",
    "# 9️⃣  Chequeo rápido: proporción de fraude en M\n",
    "print(\"\\nDistribución isFraud en M:\")\n",
    "M_df.groupBy(\"isFraud\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bb9fb",
   "metadata": {},
   "source": [
    "## 2. Construcción Train – Test  \n",
    "\n",
    "Dividimos cada partición \\(M_i\\) (definida por `isFraud` & `ProductCD`) en:\n",
    "\n",
    "| Conjunto | Propósito | Porcentaje |\n",
    "|----------|-----------|------------|\n",
    "| **Tᵣᵢ**   | Entrenamiento | 80 % |\n",
    "| **Tˢᵢ**   | Prueba        | 20 % |\n",
    "\n",
    "Pasos:\n",
    "\n",
    "1. **Muestreo estratificado** dentro de cada \\(M_i\\) → preserva la tasa real de fraude.  \n",
    "2. Verificamos que `Tᵣᵢ ∩ Tˢᵢ = ∅` y `Tᵣᵢ ∪ Tˢᵢ = M_i`.  \n",
    "3. Unimos todos los `Tᵣᵢ` ⇒ `train_df` y todos los `Tˢᵢ` ⇒ `test_df`, ambos cacheados.  \n",
    "4. Mostramos conteos globales y distribución final de fraude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ef6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_C    → Train: 2459 | Test:  623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_H    → Train: 1276 | Test:  334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_R    → Train: 1410 | Test:  375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_S    → Train:  449 | Test:  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 21:18:11 WARN DAGScheduler: Broadcasting large task binary with size 1128.1 KiB\n",
      "25/06/08 21:18:12 WARN DAGScheduler: Broadcasting large task binary with size 1137.2 KiB\n",
      "25/06/08 21:18:13 WARN DAGScheduler: Broadcasting large task binary with size 1247.3 KiB\n",
      "25/06/08 21:18:16 WARN DAGScheduler: Broadcasting large task binary with size 1137.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_W    → Train: 17221 | Test: 4383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_C    → Train:  332 | Test:   77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_H    → Train:  113 | Test:   26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_R    → Train:  140 | Test:   44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_S    → Train:  122 | Test:   27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_W    → Train:  364 | Test:   86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 21:18:49 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/06/08 21:19:07 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Train: 23,886 | Total Test: 6,080\n",
      "\n",
      "Distribución isFraud en Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|isFraud|count|\n",
      "+-------+-----+\n",
      "|      0|22815|\n",
      "|      1| 1071|\n",
      "+-------+-----+\n",
      "\n",
      "Distribución isFraud en Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 21:19:26 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|isFraud|count|\n",
      "+-------+-----+\n",
      "|      0| 5820|\n",
      "|      1|  260|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------- Sección 2 • Código: Train/Test por partición -------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "TRAIN_FRAC = 0.8\n",
    "fractions  = {0: TRAIN_FRAC, 1: TRAIN_FRAC}\n",
    "\n",
    "train_parts, test_parts = [], []\n",
    "\n",
    "for key, part_df in partitions.items():\n",
    "    # 1️⃣ Muestreo estratificado\n",
    "    tri = part_df.sampleBy(\"isFraud\", fractions, seed=42).cache()\n",
    "    tsi = part_df.subtract(tri).cache()\n",
    "    # 2️⃣ Verificación\n",
    "    assert tri.count() + tsi.count() == part_df.count()\n",
    "    assert tri.intersect(tsi).count() == 0\n",
    "    train_parts.append(tri)\n",
    "    test_parts.append(tsi)\n",
    "    print(f\"{key:6s} → Train: {tri.count():4d} | Test: {tsi.count():4d}\")\n",
    "\n",
    "# 3️⃣ Unión global\n",
    "train_df = train_parts[0]\n",
    "for df in train_parts[1:]:\n",
    "    train_df = train_df.unionByName(df)\n",
    "train_df = train_df.cache()\n",
    "\n",
    "test_df = test_parts[0]\n",
    "for df in test_parts[1:]:\n",
    "    test_df = test_df.unionByName(df)\n",
    "test_df = test_df.cache()\n",
    "\n",
    "# 4️⃣ Chequeo final\n",
    "print(f\"\\nTotal Train: {train_df.count():,} | Total Test: {test_df.count():,}\")\n",
    "print(\"\\nDistribución isFraud en Train:\")\n",
    "train_df.groupBy(\"isFraud\").count().show()\n",
    "print(\"Distribución isFraud en Test:\")\n",
    "test_df.groupBy(\"isFraud\").count().show()\n",
    "\n",
    "# Guardamos una copia completa (se usará en clustering)\n",
    "M_df = train_df.unionByName(test_df).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dea42",
   "metadata": {},
   "source": [
    "### 3. Selección de métricas para medir calidad de resultados\n",
    "\n",
    "Al trabajar con grandes volúmenes de datos y modelos de clasificación y clustering, elegimos:\n",
    "\n",
    "| Tipo | Métrica                  | Descripción y justificación |\n",
    "|------|--------------------------|------------------------------|\n",
    "| **Supervisado** | **AUC-ROC** (Area Under ROC Curve) | Mide capacidad de distinguir clases positivas/negativas en todo rango de umbrales. Escalable y robusta en datasets desbalanceados. |\n",
    "|                | **Area Under PR Curve** (PR-AUC)  | Se enfoca en precisión/recall para la clase minoritaria (fraude). Fundamental cuando la clase positiva es rara. |\n",
    "|                | **F1-Score**                      | Equilibrio entre precisión y recall; útil para puntos de decisión específicos. |\n",
    "|                | **Matriz de confusión**           | Proporciona conteos de TP, FP, FN, TN; base para métricas operativas. |\n",
    "| **No supervisado** | **Silhouette Score**             | Mide cohesión vs separación de clústeres; eficiente de calcular en Big Data. |\n",
    "|                | **Within Set Sum of Squared Errors** (WSSSE) | Evalúa compacidad interna de grupos; permite comparar distintos k. |\n",
    "|                | **Tasa de fraude por clúster**    | Medida externa: % de fraudes en cada clúster; valida relevancia de agrupamientos. |\n",
    "\n",
    "> **Nota**:  \n",
    "> - Usamos **PR-AUC** junto con AUC-ROC para asegurar que optimizamos verdaderamente la detección de fraudes.  \n",
    "> - Para clustering, complementamos Silhouette (interna) con la tasa de fraude por clúster (externa) para evaluar utilidad de los grupos.  \n",
    "> - Todas son computables eficientemente en PySpark y paralelizables sobre la muestra de ~30 k filas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80ef65",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento de Modelos de Aprendizaje  \n",
    "\n",
    "**Supervisado** – *Gradient-Boosted Trees*  \n",
    "* `StringIndexer` + `VectorAssembler` (`handleInvalid=\"keep\"`).  \n",
    "* Grid ligero (`maxDepth` 5 vs 8) validado con `TrainValidationSplit`.  \n",
    "* Métricas: **AUC-ROC** & **PR-AUC**.\n",
    "\n",
    "**No supervisado** – *Gaussian Mixture*  \n",
    "* Variables numéricas escaladas (`StandardScaler`).  \n",
    "* `k = 3` componentes.  \n",
    "* Métricas: **Silhouette** & % fraude por componente.\n",
    "\n",
    "Todos los `NaN` numéricos se rellenan con −999 👉 cero errores en Spark.  \n",
    "`shuffle.partitions = 200` y `parallelism = 4` usan al máximo la CPU del M4 Pro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12ba1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexaremos 30 columnas categóricas; omitimos 1 muy grandes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 21:20:49 WARN DAGScheduler: Broadcasting large task binary with size 1102.1 KiB\n",
      "25/06/08 21:20:49 WARN DAGScheduler: Broadcasting large task binary with size 1102.1 KiB\n",
      "25/06/08 21:21:00 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:02 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:06 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:06 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:11 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:11 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:15 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:15 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:20 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:20 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:25 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:25 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:29 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:29 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:33 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:33 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:37 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:37 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:41 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:41 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:46 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:46 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:50 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:50 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:54 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:54 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:58 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:21:58 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:02 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:02 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:07 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:07 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:11 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:11 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:16 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:16 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:20 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:20 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:24 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:24 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:29 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:29 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:34 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:34 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:38 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:38 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:43 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:43 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:48 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:48 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:53 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:53 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:58 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:22:58 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:03 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:03 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:08 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:08 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:13 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:13 WARN DAGScheduler: Broadcasting large task binary with size 1151.0 KiB\n",
      "25/06/08 21:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1472.6 KiB\n",
      "25/06/08 21:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1472.6 KiB\n",
      "25/06/08 21:23:19 WARN DAGScheduler: Broadcasting large task binary with size 1472.4 KiB\n",
      "25/06/08 21:23:19 WARN DAGScheduler: Broadcasting large task binary with size 1472.4 KiB\n",
      "25/06/08 21:24:09 WARN DAGScheduler: Broadcasting large task binary with size 1479.7 KiB\n",
      "25/06/08 21:24:59 WARN DAGScheduler: Broadcasting large task binary with size 1479.7 KiB\n",
      "25/06/08 21:25:58 WARN DAGScheduler: Broadcasting large task binary with size 1633.0 KiB\n",
      "25/06/08 21:26:55 WARN DAGScheduler: Broadcasting large task binary with size 1633.0 KiB\n",
      "25/06/08 21:27:46 WARN DAGScheduler: Broadcasting large task binary with size 1633.7 KiB\n",
      "25/06/08 21:28:37 WARN DAGScheduler: Broadcasting large task binary with size 1633.7 KiB\n",
      "25/06/08 21:28:39 WARN DAGScheduler: Broadcasting large task binary with size 1634.8 KiB\n",
      "25/06/08 21:28:42 WARN DAGScheduler: Broadcasting large task binary with size 1634.8 KiB\n",
      "25/06/08 21:28:46 WARN DAGScheduler: Broadcasting large task binary with size 1636.1 KiB\n",
      "25/06/08 21:28:49 WARN DAGScheduler: Broadcasting large task binary with size 1636.1 KiB\n",
      "25/06/08 21:28:53 WARN DAGScheduler: Broadcasting large task binary with size 1643.8 KiB\n",
      "25/06/08 21:28:57 WARN DAGScheduler: Broadcasting large task binary with size 1639.0 KiB\n",
      "25/06/08 21:29:00 WARN DAGScheduler: Broadcasting large task binary with size 1644.3 KiB\n",
      "25/06/08 21:29:04 WARN DAGScheduler: Broadcasting large task binary with size 1642.5 KiB\n",
      "25/06/08 21:29:07 WARN DAGScheduler: Broadcasting large task binary with size 1645.3 KiB\n",
      "25/06/08 21:29:11 WARN DAGScheduler: Broadcasting large task binary with size 1649.3 KiB\n",
      "25/06/08 21:29:15 WARN DAGScheduler: Broadcasting large task binary with size 1647.6 KiB\n",
      "25/06/08 21:29:19 WARN DAGScheduler: Broadcasting large task binary with size 1649.8 KiB\n",
      "25/06/08 21:29:22 WARN DAGScheduler: Broadcasting large task binary with size 1652.0 KiB\n",
      "25/06/08 21:29:26 WARN DAGScheduler: Broadcasting large task binary with size 1650.7 KiB\n",
      "25/06/08 21:29:29 WARN DAGScheduler: Broadcasting large task binary with size 1652.5 KiB\n",
      "25/06/08 21:29:32 WARN DAGScheduler: Broadcasting large task binary with size 1653.4 KiB\n",
      "25/06/08 21:29:35 WARN DAGScheduler: Broadcasting large task binary with size 1653.1 KiB\n",
      "25/06/08 21:29:39 WARN DAGScheduler: Broadcasting large task binary with size 1656.1 KiB\n",
      "25/06/08 21:29:43 WARN DAGScheduler: Broadcasting large task binary with size 1656.1 KiB\n",
      "25/06/08 21:29:48 WARN DAGScheduler: Broadcasting large task binary with size 1660.5 KiB\n",
      "25/06/08 21:29:52 WARN DAGScheduler: Broadcasting large task binary with size 1658.9 KiB\n",
      "25/06/08 21:29:58 WARN DAGScheduler: Broadcasting large task binary with size 1663.9 KiB\n",
      "25/06/08 21:30:01 WARN DAGScheduler: Broadcasting large task binary with size 1659.4 KiB\n",
      "25/06/08 21:30:04 WARN DAGScheduler: Broadcasting large task binary with size 1664.4 KiB\n",
      "25/06/08 21:30:08 WARN DAGScheduler: Broadcasting large task binary with size 1660.4 KiB\n",
      "25/06/08 21:30:11 WARN DAGScheduler: Broadcasting large task binary with size 1665.3 KiB\n",
      "25/06/08 21:30:14 WARN DAGScheduler: Broadcasting large task binary with size 1662.5 KiB\n",
      "25/06/08 21:30:18 WARN DAGScheduler: Broadcasting large task binary with size 1667.6 KiB\n",
      "25/06/08 21:30:22 WARN DAGScheduler: Broadcasting large task binary with size 1666.7 KiB\n",
      "25/06/08 21:30:26 WARN DAGScheduler: Broadcasting large task binary with size 1670.6 KiB\n",
      "25/06/08 21:30:30 WARN DAGScheduler: Broadcasting large task binary with size 1667.2 KiB\n",
      "25/06/08 21:30:36 WARN DAGScheduler: Broadcasting large task binary with size 1676.6 KiB\n",
      "25/06/08 21:30:39 WARN DAGScheduler: Broadcasting large task binary with size 1668.3 KiB\n",
      "25/06/08 21:30:45 WARN DAGScheduler: Broadcasting large task binary with size 1678.9 KiB\n",
      "25/06/08 21:30:49 WARN DAGScheduler: Broadcasting large task binary with size 1670.4 KiB\n",
      "25/06/08 21:30:52 WARN DAGScheduler: Broadcasting large task binary with size 1679.4 KiB\n",
      "25/06/08 21:30:56 WARN DAGScheduler: Broadcasting large task binary with size 1673.5 KiB\n",
      "25/06/08 21:30:59 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "25/06/08 21:31:03 WARN DAGScheduler: Broadcasting large task binary with size 1674.0 KiB\n",
      "25/06/08 21:31:06 WARN DAGScheduler: Broadcasting large task binary with size 1681.6 KiB\n",
      "25/06/08 21:31:10 WARN DAGScheduler: Broadcasting large task binary with size 1674.9 KiB\n",
      "25/06/08 21:31:14 WARN DAGScheduler: Broadcasting large task binary with size 1683.8 KiB\n",
      "25/06/08 21:31:17 WARN DAGScheduler: Broadcasting large task binary with size 1677.4 KiB\n",
      "25/06/08 21:31:22 WARN DAGScheduler: Broadcasting large task binary with size 1687.6 KiB\n",
      "25/06/08 21:31:27 WARN DAGScheduler: Broadcasting large task binary with size 1680.4 KiB\n",
      "25/06/08 21:31:34 WARN DAGScheduler: Broadcasting large task binary with size 1691.8 KiB\n",
      "25/06/08 21:31:37 WARN DAGScheduler: Broadcasting large task binary with size 1681.4 KiB\n",
      "25/06/08 21:31:41 WARN DAGScheduler: Broadcasting large task binary with size 1692.3 KiB\n",
      "25/06/08 21:31:44 WARN DAGScheduler: Broadcasting large task binary with size 1682.9 KiB\n",
      "25/06/08 21:31:47 WARN DAGScheduler: Broadcasting large task binary with size 1692.9 KiB\n",
      "25/06/08 21:31:51 WARN DAGScheduler: Broadcasting large task binary with size 1684.8 KiB\n",
      "25/06/08 21:31:54 WARN DAGScheduler: Broadcasting large task binary with size 1694.4 KiB\n",
      "25/06/08 21:31:58 WARN DAGScheduler: Broadcasting large task binary with size 1688.2 KiB\n",
      "25/06/08 21:32:02 WARN DAGScheduler: Broadcasting large task binary with size 1697.0 KiB\n",
      "25/06/08 21:32:06 WARN DAGScheduler: Broadcasting large task binary with size 1688.7 KiB\n",
      "25/06/08 21:32:10 WARN DAGScheduler: Broadcasting large task binary with size 1700.8 KiB\n",
      "25/06/08 21:32:14 WARN DAGScheduler: Broadcasting large task binary with size 1689.7 KiB\n",
      "25/06/08 21:32:20 WARN DAGScheduler: Broadcasting large task binary with size 1705.0 KiB\n",
      "25/06/08 21:32:24 WARN DAGScheduler: Broadcasting large task binary with size 1691.9 KiB\n",
      "25/06/08 21:32:28 WARN DAGScheduler: Broadcasting large task binary with size 1705.5 KiB\n",
      "25/06/08 21:32:32 WARN DAGScheduler: Broadcasting large task binary with size 1695.3 KiB\n",
      "25/06/08 21:32:35 WARN DAGScheduler: Broadcasting large task binary with size 1706.6 KiB\n",
      "25/06/08 21:32:39 WARN DAGScheduler: Broadcasting large task binary with size 1695.7 KiB\n",
      "25/06/08 21:32:42 WARN DAGScheduler: Broadcasting large task binary with size 1707.8 KiB\n",
      "25/06/08 21:32:46 WARN DAGScheduler: Broadcasting large task binary with size 1697.1 KiB\n",
      "25/06/08 21:32:50 WARN DAGScheduler: Broadcasting large task binary with size 1712.2 KiB\n",
      "25/06/08 21:32:54 WARN DAGScheduler: Broadcasting large task binary with size 1699.2 KiB\n",
      "25/06/08 21:33:00 WARN DAGScheduler: Broadcasting large task binary with size 1716.6 KiB\n",
      "25/06/08 21:33:04 WARN DAGScheduler: Broadcasting large task binary with size 1702.3 KiB\n",
      "25/06/08 21:33:10 WARN DAGScheduler: Broadcasting large task binary with size 1720.5 KiB\n",
      "25/06/08 21:33:14 WARN DAGScheduler: Broadcasting large task binary with size 1702.7 KiB\n",
      "25/06/08 21:33:17 WARN DAGScheduler: Broadcasting large task binary with size 1720.9 KiB\n",
      "25/06/08 21:33:20 WARN DAGScheduler: Broadcasting large task binary with size 1703.7 KiB\n",
      "25/06/08 21:33:24 WARN DAGScheduler: Broadcasting large task binary with size 1722.8 KiB\n",
      "25/06/08 21:33:28 WARN DAGScheduler: Broadcasting large task binary with size 1705.5 KiB\n",
      "25/06/08 21:33:32 WARN DAGScheduler: Broadcasting large task binary with size 1724.6 KiB\n",
      "25/06/08 21:33:36 WARN DAGScheduler: Broadcasting large task binary with size 1709.5 KiB\n",
      "25/06/08 21:33:41 WARN DAGScheduler: Broadcasting large task binary with size 1727.3 KiB\n",
      "25/06/08 21:33:44 WARN DAGScheduler: Broadcasting large task binary with size 1710.5 KiB\n",
      "25/06/08 21:33:50 WARN DAGScheduler: Broadcasting large task binary with size 1732.3 KiB\n",
      "25/06/08 21:33:53 WARN DAGScheduler: Broadcasting large task binary with size 1712.0 KiB\n",
      "25/06/08 21:34:00 WARN DAGScheduler: Broadcasting large task binary with size 1736.3 KiB\n",
      "25/06/08 21:34:04 WARN DAGScheduler: Broadcasting large task binary with size 1713.7 KiB\n",
      "25/06/08 21:34:07 WARN DAGScheduler: Broadcasting large task binary with size 1736.8 KiB\n",
      "25/06/08 21:34:11 WARN DAGScheduler: Broadcasting large task binary with size 1717.0 KiB\n",
      "25/06/08 21:34:15 WARN DAGScheduler: Broadcasting large task binary with size 1737.3 KiB\n",
      "25/06/08 21:34:18 WARN DAGScheduler: Broadcasting large task binary with size 1717.5 KiB\n",
      "25/06/08 21:34:22 WARN DAGScheduler: Broadcasting large task binary with size 1739.1 KiB\n",
      "25/06/08 21:34:25 WARN DAGScheduler: Broadcasting large task binary with size 1718.3 KiB\n",
      "25/06/08 21:34:29 WARN DAGScheduler: Broadcasting large task binary with size 1741.4 KiB\n",
      "25/06/08 21:34:33 WARN DAGScheduler: Broadcasting large task binary with size 1719.5 KiB\n",
      "25/06/08 21:34:38 WARN DAGScheduler: Broadcasting large task binary with size 1745.9 KiB\n",
      "25/06/08 21:34:43 WARN DAGScheduler: Broadcasting large task binary with size 1722.0 KiB\n",
      "25/06/08 21:34:49 WARN DAGScheduler: Broadcasting large task binary with size 1749.3 KiB\n",
      "25/06/08 21:34:52 WARN DAGScheduler: Broadcasting large task binary with size 1723.4 KiB\n",
      "25/06/08 21:34:55 WARN DAGScheduler: Broadcasting large task binary with size 1749.8 KiB\n",
      "25/06/08 21:34:59 WARN DAGScheduler: Broadcasting large task binary with size 1724.0 KiB\n",
      "25/06/08 21:35:02 WARN DAGScheduler: Broadcasting large task binary with size 1751.2 KiB\n",
      "25/06/08 21:35:06 WARN DAGScheduler: Broadcasting large task binary with size 1725.5 KiB\n",
      "25/06/08 21:35:09 WARN DAGScheduler: Broadcasting large task binary with size 1752.4 KiB\n",
      "25/06/08 21:35:14 WARN DAGScheduler: Broadcasting large task binary with size 1728.5 KiB\n",
      "25/06/08 21:35:18 WARN DAGScheduler: Broadcasting large task binary with size 1755.7 KiB\n",
      "25/06/08 21:35:22 WARN DAGScheduler: Broadcasting large task binary with size 1729.5 KiB\n",
      "25/06/08 21:35:28 WARN DAGScheduler: Broadcasting large task binary with size 1761.3 KiB\n",
      "25/06/08 21:35:31 WARN DAGScheduler: Broadcasting large task binary with size 1730.0 KiB\n",
      "25/06/08 21:35:38 WARN DAGScheduler: Broadcasting large task binary with size 1765.9 KiB\n",
      "25/06/08 21:35:42 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "25/06/08 21:35:46 WARN DAGScheduler: Broadcasting large task binary with size 1766.4 KiB\n",
      "25/06/08 21:35:50 WARN DAGScheduler: Broadcasting large task binary with size 1736.5 KiB\n",
      "25/06/08 21:35:53 WARN DAGScheduler: Broadcasting large task binary with size 1766.9 KiB\n",
      "25/06/08 21:35:56 WARN DAGScheduler: Broadcasting large task binary with size 1737.0 KiB\n",
      "25/06/08 21:36:00 WARN DAGScheduler: Broadcasting large task binary with size 1768.7 KiB\n",
      "25/06/08 21:36:03 WARN DAGScheduler: Broadcasting large task binary with size 1737.9 KiB\n",
      "25/06/08 21:36:08 WARN DAGScheduler: Broadcasting large task binary with size 1772.0 KiB\n",
      "25/06/08 21:36:12 WARN DAGScheduler: Broadcasting large task binary with size 1739.7 KiB\n",
      "25/06/08 21:36:16 WARN DAGScheduler: Broadcasting large task binary with size 1775.5 KiB\n",
      "25/06/08 21:36:21 WARN DAGScheduler: Broadcasting large task binary with size 1744.3 KiB\n",
      "25/06/08 21:36:28 WARN DAGScheduler: Broadcasting large task binary with size 1778.1 KiB\n",
      "25/06/08 21:36:32 WARN DAGScheduler: Broadcasting large task binary with size 1744.8 KiB\n",
      "25/06/08 21:36:36 WARN DAGScheduler: Broadcasting large task binary with size 1778.6 KiB\n",
      "25/06/08 21:36:39 WARN DAGScheduler: Broadcasting large task binary with size 1745.4 KiB\n",
      "25/06/08 21:36:42 WARN DAGScheduler: Broadcasting large task binary with size 1779.2 KiB\n",
      "25/06/08 21:36:46 WARN DAGScheduler: Broadcasting large task binary with size 1747.2 KiB\n",
      "25/06/08 21:36:50 WARN DAGScheduler: Broadcasting large task binary with size 1780.7 KiB\n",
      "25/06/08 21:36:54 WARN DAGScheduler: Broadcasting large task binary with size 1750.3 KiB\n",
      "25/06/08 21:36:59 WARN DAGScheduler: Broadcasting large task binary with size 1783.0 KiB\n",
      "25/06/08 21:37:02 WARN DAGScheduler: Broadcasting large task binary with size 1750.8 KiB\n",
      "25/06/08 21:37:07 WARN DAGScheduler: Broadcasting large task binary with size 1788.3 KiB\n",
      "25/06/08 21:37:12 WARN DAGScheduler: Broadcasting large task binary with size 1751.3 KiB\n",
      "25/06/08 21:37:18 WARN DAGScheduler: Broadcasting large task binary with size 1792.5 KiB\n",
      "25/06/08 21:37:22 WARN DAGScheduler: Broadcasting large task binary with size 1752.5 KiB\n",
      "25/06/08 21:37:26 WARN DAGScheduler: Broadcasting large task binary with size 1793.5 KiB\n",
      "25/06/08 21:37:29 WARN DAGScheduler: Broadcasting large task binary with size 1756.0 KiB\n",
      "25/06/08 21:37:33 WARN DAGScheduler: Broadcasting large task binary with size 1795.0 KiB\n",
      "25/06/08 21:37:36 WARN DAGScheduler: Broadcasting large task binary with size 1756.5 KiB\n",
      "25/06/08 21:37:40 WARN DAGScheduler: Broadcasting large task binary with size 1796.3 KiB\n",
      "25/06/08 21:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1757.1 KiB\n",
      "25/06/08 21:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1800.0 KiB\n",
      "25/06/08 21:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1758.8 KiB\n",
      "25/06/08 21:37:59 WARN DAGScheduler: Broadcasting large task binary with size 1806.0 KiB\n",
      "25/06/08 21:38:03 WARN DAGScheduler: Broadcasting large task binary with size 1762.3 KiB\n",
      "25/06/08 21:38:11 WARN DAGScheduler: Broadcasting large task binary with size 1810.6 KiB\n",
      "25/06/08 21:38:15 WARN DAGScheduler: Broadcasting large task binary with size 1762.8 KiB\n",
      "25/06/08 21:38:18 WARN DAGScheduler: Broadcasting large task binary with size 1811.1 KiB\n",
      "25/06/08 21:38:22 WARN DAGScheduler: Broadcasting large task binary with size 1763.3 KiB\n",
      "25/06/08 21:38:25 WARN DAGScheduler: Broadcasting large task binary with size 1812.7 KiB\n",
      "25/06/08 21:38:29 WARN DAGScheduler: Broadcasting large task binary with size 1765.2 KiB\n",
      "25/06/08 21:38:33 WARN DAGScheduler: Broadcasting large task binary with size 1814.9 KiB\n",
      "25/06/08 21:38:37 WARN DAGScheduler: Broadcasting large task binary with size 1767.4 KiB\n",
      "25/06/08 21:38:42 WARN DAGScheduler: Broadcasting large task binary with size 1817.4 KiB\n",
      "25/06/08 21:38:46 WARN DAGScheduler: Broadcasting large task binary with size 1767.9 KiB\n",
      "25/06/08 21:38:51 WARN DAGScheduler: Broadcasting large task binary with size 1822.1 KiB\n",
      "25/06/08 21:38:55 WARN DAGScheduler: Broadcasting large task binary with size 1769.1 KiB\n",
      "25/06/08 21:39:02 WARN DAGScheduler: Broadcasting large task binary with size 1824.6 KiB\n",
      "25/06/08 21:39:06 WARN DAGScheduler: Broadcasting large task binary with size 1770.3 KiB\n",
      "25/06/08 21:39:10 WARN DAGScheduler: Broadcasting large task binary with size 1825.1 KiB\n",
      "25/06/08 21:39:14 WARN DAGScheduler: Broadcasting large task binary with size 1773.4 KiB\n",
      "25/06/08 21:39:18 WARN DAGScheduler: Broadcasting large task binary with size 1826.6 KiB\n",
      "25/06/08 21:39:22 WARN DAGScheduler: Broadcasting large task binary with size 1774.6 KiB\n",
      "25/06/08 21:39:26 WARN DAGScheduler: Broadcasting large task binary with size 1828.2 KiB\n",
      "25/06/08 21:39:30 WARN DAGScheduler: Broadcasting large task binary with size 1775.9 KiB\n",
      "25/06/08 21:39:34 WARN DAGScheduler: Broadcasting large task binary with size 1832.4 KiB\n",
      "25/06/08 21:39:38 WARN DAGScheduler: Broadcasting large task binary with size 1778.0 KiB\n",
      "25/06/08 21:39:45 WARN DAGScheduler: Broadcasting large task binary with size 1839.1 KiB\n",
      "25/06/08 21:39:49 WARN DAGScheduler: Broadcasting large task binary with size 1780.5 KiB\n",
      "25/06/08 21:39:56 WARN DAGScheduler: Broadcasting large task binary with size 1842.3 KiB\n",
      "25/06/08 21:40:00 WARN DAGScheduler: Broadcasting large task binary with size 1781.0 KiB\n",
      "25/06/08 21:40:04 WARN DAGScheduler: Broadcasting large task binary with size 1842.8 KiB\n",
      "25/06/08 21:40:07 WARN DAGScheduler: Broadcasting large task binary with size 1781.6 KiB\n",
      "25/06/08 21:40:10 WARN DAGScheduler: Broadcasting large task binary with size 1843.3 KiB\n",
      "25/06/08 21:40:14 WARN DAGScheduler: Broadcasting large task binary with size 1782.8 KiB\n",
      "25/06/08 21:40:18 WARN DAGScheduler: Broadcasting large task binary with size 1845.1 KiB\n",
      "25/06/08 21:40:22 WARN DAGScheduler: Broadcasting large task binary with size 1785.1 KiB\n",
      "25/06/08 21:40:26 WARN DAGScheduler: Broadcasting large task binary with size 1849.1 KiB\n",
      "25/06/08 21:40:30 WARN DAGScheduler: Broadcasting large task binary with size 1785.5 KiB\n",
      "25/06/08 21:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1853.0 KiB\n",
      "25/06/08 21:40:39 WARN DAGScheduler: Broadcasting large task binary with size 1786.4 KiB\n",
      "25/06/08 21:40:45 WARN DAGScheduler: Broadcasting large task binary with size 1854.6 KiB\n",
      "25/06/08 21:40:49 WARN DAGScheduler: Broadcasting large task binary with size 1788.4 KiB\n",
      "25/06/08 21:40:53 WARN DAGScheduler: Broadcasting large task binary with size 1855.1 KiB\n",
      "25/06/08 21:40:57 WARN DAGScheduler: Broadcasting large task binary with size 1793.0 KiB\n",
      "25/06/08 21:41:00 WARN DAGScheduler: Broadcasting large task binary with size 1855.7 KiB\n",
      "25/06/08 21:41:04 WARN DAGScheduler: Broadcasting large task binary with size 1793.5 KiB\n",
      "25/06/08 21:41:07 WARN DAGScheduler: Broadcasting large task binary with size 1858.3 KiB\n",
      "25/06/08 21:41:11 WARN DAGScheduler: Broadcasting large task binary with size 1794.7 KiB\n",
      "25/06/08 21:41:15 WARN DAGScheduler: Broadcasting large task binary with size 1862.0 KiB\n",
      "25/06/08 21:41:19 WARN DAGScheduler: Broadcasting large task binary with size 1795.9 KiB\n",
      "25/06/08 21:41:25 WARN DAGScheduler: Broadcasting large task binary with size 1869.0 KiB\n",
      "25/06/08 21:41:29 WARN DAGScheduler: Broadcasting large task binary with size 1799.0 KiB\n",
      "25/06/08 21:41:37 WARN DAGScheduler: Broadcasting large task binary with size 1872.6 KiB\n",
      "25/06/08 21:41:41 WARN DAGScheduler: Broadcasting large task binary with size 1799.5 KiB\n",
      "25/06/08 21:41:44 WARN DAGScheduler: Broadcasting large task binary with size 1873.5 KiB\n",
      "25/06/08 21:41:47 WARN DAGScheduler: Broadcasting large task binary with size 1800.7 KiB\n",
      "25/06/08 21:41:51 WARN DAGScheduler: Broadcasting large task binary with size 1874.1 KiB\n",
      "25/06/08 21:41:55 WARN DAGScheduler: Broadcasting large task binary with size 1802.5 KiB\n",
      "25/06/08 21:41:58 WARN DAGScheduler: Broadcasting large task binary with size 1877.6 KiB\n",
      "25/06/08 21:42:04 WARN DAGScheduler: Broadcasting large task binary with size 1102.1 KiB\n",
      "25/06/08 21:42:07 WARN DAGScheduler: Broadcasting large task binary with size 1880.8 KiB\n",
      "25/06/08 21:42:19 WARN DAGScheduler: Broadcasting large task binary with size 1639.7 KiB\n",
      "25/06/08 21:42:23 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/06/08 21:42:25 WARN DAGScheduler: Broadcasting large task binary with size 1886.2 KiB\n",
      "25/06/08 21:43:22 WARN DAGScheduler: Broadcasting large task binary with size 1891.2 KiB\n",
      "25/06/08 21:43:26 WARN DAGScheduler: Broadcasting large task binary with size 1891.7 KiB\n",
      "25/06/08 21:43:30 WARN DAGScheduler: Broadcasting large task binary with size 1892.8 KiB\n",
      "25/06/08 21:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1894.7 KiB\n",
      "25/06/08 21:43:39 WARN DAGScheduler: Broadcasting large task binary with size 1897.1 KiB\n",
      "25/06/08 21:43:45 WARN DAGScheduler: Broadcasting large task binary with size 1901.4 KiB\n",
      "25/06/08 21:43:56 WARN DAGScheduler: Broadcasting large task binary with size 1904.3 KiB\n",
      "25/06/08 21:46:55 WARN DAGScheduler: Broadcasting large task binary with size 1904.8 KiB\n",
      "25/06/08 21:47:00 WARN DAGScheduler: Broadcasting large task binary with size 1906.1 KiB\n",
      "25/06/08 21:47:04 WARN DAGScheduler: Broadcasting large task binary with size 1907.8 KiB\n",
      "25/06/08 21:47:08 WARN DAGScheduler: Broadcasting large task binary with size 1911.5 KiB\n",
      "25/06/08 21:47:14 WARN DAGScheduler: Broadcasting large task binary with size 1916.6 KiB\n",
      "25/06/08 21:47:21 WARN DAGScheduler: Broadcasting large task binary with size 1920.3 KiB\n",
      "25/06/08 21:47:24 WARN DAGScheduler: Broadcasting large task binary with size 1921.3 KiB\n",
      "25/06/08 21:47:28 WARN DAGScheduler: Broadcasting large task binary with size 1921.9 KiB\n",
      "25/06/08 21:47:32 WARN DAGScheduler: Broadcasting large task binary with size 1924.1 KiB\n",
      "25/06/08 21:47:36 WARN DAGScheduler: Broadcasting large task binary with size 1926.6 KiB\n",
      "25/06/08 21:47:41 WARN DAGScheduler: Broadcasting large task binary with size 1934.3 KiB\n",
      "25/06/08 21:47:48 WARN DAGScheduler: Broadcasting large task binary with size 1938.3 KiB\n",
      "25/06/08 21:47:51 WARN DAGScheduler: Broadcasting large task binary with size 1938.8 KiB\n",
      "25/06/08 21:47:55 WARN DAGScheduler: Broadcasting large task binary with size 1939.7 KiB\n",
      "25/06/08 21:47:58 WARN DAGScheduler: Broadcasting large task binary with size 1941.6 KiB\n",
      "25/06/08 21:48:02 WARN DAGScheduler: Broadcasting large task binary with size 1943.9 KiB\n",
      "25/06/08 21:48:08 WARN DAGScheduler: Broadcasting large task binary with size 1949.6 KiB\n",
      "25/06/08 21:48:16 WARN DAGScheduler: Broadcasting large task binary with size 1953.4 KiB\n",
      "25/06/08 21:48:20 WARN DAGScheduler: Broadcasting large task binary with size 1953.9 KiB\n",
      "25/06/08 21:48:23 WARN DAGScheduler: Broadcasting large task binary with size 1955.1 KiB\n",
      "25/06/08 21:48:27 WARN DAGScheduler: Broadcasting large task binary with size 1956.3 KiB\n",
      "25/06/08 21:48:31 WARN DAGScheduler: Broadcasting large task binary with size 1958.6 KiB\n",
      "25/06/08 21:48:37 WARN DAGScheduler: Broadcasting large task binary with size 1964.7 KiB\n",
      "25/06/08 21:48:43 WARN DAGScheduler: Broadcasting large task binary with size 1967.5 KiB\n",
      "25/06/08 21:48:47 WARN DAGScheduler: Broadcasting large task binary with size 1968.0 KiB\n",
      "25/06/08 21:48:50 WARN DAGScheduler: Broadcasting large task binary with size 1968.9 KiB\n",
      "25/06/08 21:48:54 WARN DAGScheduler: Broadcasting large task binary with size 1971.7 KiB\n",
      "25/06/08 21:48:58 WARN DAGScheduler: Broadcasting large task binary with size 1975.5 KiB\n",
      "25/06/08 21:49:04 WARN DAGScheduler: Broadcasting large task binary with size 1981.2 KiB\n",
      "25/06/08 21:49:11 WARN DAGScheduler: Broadcasting large task binary with size 1984.2 KiB\n",
      "25/06/08 21:49:15 WARN DAGScheduler: Broadcasting large task binary with size 1984.7 KiB\n",
      "25/06/08 21:49:19 WARN DAGScheduler: Broadcasting large task binary with size 1985.9 KiB\n",
      "25/06/08 21:49:23 WARN DAGScheduler: Broadcasting large task binary with size 1987.1 KiB\n",
      "25/06/08 21:49:27 WARN DAGScheduler: Broadcasting large task binary with size 1989.4 KiB\n",
      "25/06/08 21:49:34 WARN DAGScheduler: Broadcasting large task binary with size 1994.8 KiB\n",
      "25/06/08 21:49:41 WARN DAGScheduler: Broadcasting large task binary with size 1997.2 KiB\n",
      "25/06/08 21:49:45 WARN DAGScheduler: Broadcasting large task binary with size 1997.7 KiB\n",
      "25/06/08 21:49:49 WARN DAGScheduler: Broadcasting large task binary with size 1998.8 KiB\n",
      "25/06/08 21:49:53 WARN DAGScheduler: Broadcasting large task binary with size 2000.7 KiB\n",
      "25/06/08 21:49:57 WARN DAGScheduler: Broadcasting large task binary with size 2003.3 KiB\n",
      "25/06/08 21:50:03 WARN DAGScheduler: Broadcasting large task binary with size 2008.7 KiB\n",
      "25/06/08 21:50:11 WARN DAGScheduler: Broadcasting large task binary with size 1846.7 KiB\n",
      "25/06/08 21:52:16 WARN DAGScheduler: Broadcasting large task binary with size 1441.2 KiB\n",
      "25/06/08 21:52:16 WARN DAGScheduler: Broadcasting large task binary with size 1441.0 KiB\n",
      "25/06/08 21:53:14 WARN DAGScheduler: Broadcasting large task binary with size 1448.3 KiB\n",
      "25/06/08 21:54:23 WARN DAGScheduler: Broadcasting large task binary with size 1601.8 KiB\n",
      "25/06/08 21:55:22 WARN DAGScheduler: Broadcasting large task binary with size 1602.6 KiB\n",
      "25/06/08 21:55:26 WARN DAGScheduler: Broadcasting large task binary with size 1603.7 KiB\n",
      "25/06/08 21:55:29 WARN DAGScheduler: Broadcasting large task binary with size 1605.3 KiB\n",
      "25/06/08 21:55:34 WARN DAGScheduler: Broadcasting large task binary with size 1608.1 KiB\n",
      "25/06/08 21:55:39 WARN DAGScheduler: Broadcasting large task binary with size 1612.2 KiB\n",
      "25/06/08 21:55:45 WARN DAGScheduler: Broadcasting large task binary with size 1619.4 KiB\n",
      "25/06/08 21:55:49 WARN DAGScheduler: Broadcasting large task binary with size 1619.9 KiB\n",
      "25/06/08 21:55:52 WARN DAGScheduler: Broadcasting large task binary with size 1620.9 KiB\n",
      "25/06/08 21:55:56 WARN DAGScheduler: Broadcasting large task binary with size 1622.2 KiB\n",
      "25/06/08 21:56:00 WARN DAGScheduler: Broadcasting large task binary with size 1626.4 KiB\n",
      "25/06/08 21:56:06 WARN DAGScheduler: Broadcasting large task binary with size 1630.9 KiB\n",
      "25/06/08 21:56:13 WARN DAGScheduler: Broadcasting large task binary with size 1634.6 KiB\n",
      "25/06/08 21:56:16 WARN DAGScheduler: Broadcasting large task binary with size 1635.1 KiB\n",
      "25/06/08 21:56:20 WARN DAGScheduler: Broadcasting large task binary with size 1636.2 KiB\n",
      "25/06/08 21:56:24 WARN DAGScheduler: Broadcasting large task binary with size 1639.1 KiB\n",
      "25/06/08 21:56:28 WARN DAGScheduler: Broadcasting large task binary with size 1643.6 KiB\n",
      "25/06/08 21:56:34 WARN DAGScheduler: Broadcasting large task binary with size 1649.8 KiB\n",
      "25/06/08 21:56:42 WARN DAGScheduler: Broadcasting large task binary with size 1652.4 KiB\n",
      "25/06/08 21:56:46 WARN DAGScheduler: Broadcasting large task binary with size 1652.8 KiB\n",
      "25/06/08 21:56:49 WARN DAGScheduler: Broadcasting large task binary with size 1653.9 KiB\n",
      "25/06/08 21:56:53 WARN DAGScheduler: Broadcasting large task binary with size 1656.8 KiB\n",
      "25/06/08 21:56:57 WARN DAGScheduler: Broadcasting large task binary with size 1661.1 KiB\n",
      "25/06/08 21:57:03 WARN DAGScheduler: Broadcasting large task binary with size 1667.3 KiB\n",
      "25/06/08 21:57:11 WARN DAGScheduler: Broadcasting large task binary with size 1671.3 KiB\n",
      "25/06/08 21:57:14 WARN DAGScheduler: Broadcasting large task binary with size 1671.7 KiB\n",
      "25/06/08 21:57:18 WARN DAGScheduler: Broadcasting large task binary with size 1672.8 KiB\n",
      "25/06/08 21:57:21 WARN DAGScheduler: Broadcasting large task binary with size 1674.9 KiB\n",
      "25/06/08 21:57:26 WARN DAGScheduler: Broadcasting large task binary with size 1678.3 KiB\n",
      "25/06/08 21:57:32 WARN DAGScheduler: Broadcasting large task binary with size 1683.9 KiB\n",
      "25/06/08 21:57:39 WARN DAGScheduler: Broadcasting large task binary with size 1687.1 KiB\n",
      "25/06/08 21:57:42 WARN DAGScheduler: Broadcasting large task binary with size 1687.5 KiB\n",
      "25/06/08 21:57:46 WARN DAGScheduler: Broadcasting large task binary with size 1688.6 KiB\n",
      "25/06/08 21:57:50 WARN DAGScheduler: Broadcasting large task binary with size 1689.9 KiB\n",
      "25/06/08 21:57:54 WARN DAGScheduler: Broadcasting large task binary with size 1692.9 KiB\n",
      "25/06/08 21:58:00 WARN DAGScheduler: Broadcasting large task binary with size 1700.5 KiB\n",
      "25/06/08 21:58:08 WARN DAGScheduler: Broadcasting large task binary with size 1704.5 KiB\n",
      "25/06/08 21:58:11 WARN DAGScheduler: Broadcasting large task binary with size 1705.0 KiB\n",
      "25/06/08 21:58:15 WARN DAGScheduler: Broadcasting large task binary with size 1706.4 KiB\n",
      "25/06/08 21:58:19 WARN DAGScheduler: Broadcasting large task binary with size 1707.6 KiB\n",
      "25/06/08 21:58:23 WARN DAGScheduler: Broadcasting large task binary with size 1710.5 KiB\n",
      "25/06/08 21:58:29 WARN DAGScheduler: Broadcasting large task binary with size 1716.5 KiB\n",
      "25/06/08 21:58:36 WARN DAGScheduler: Broadcasting large task binary with size 1721.0 KiB\n",
      "25/06/08 21:58:40 WARN DAGScheduler: Broadcasting large task binary with size 1721.5 KiB\n",
      "25/06/08 21:58:43 WARN DAGScheduler: Broadcasting large task binary with size 1722.1 KiB\n",
      "25/06/08 21:58:47 WARN DAGScheduler: Broadcasting large task binary with size 1723.3 KiB\n",
      "25/06/08 21:58:51 WARN DAGScheduler: Broadcasting large task binary with size 1725.9 KiB\n",
      "25/06/08 21:58:57 WARN DAGScheduler: Broadcasting large task binary with size 1730.5 KiB\n",
      "25/06/08 21:59:03 WARN DAGScheduler: Broadcasting large task binary with size 1734.2 KiB\n",
      "25/06/08 21:59:07 WARN DAGScheduler: Broadcasting large task binary with size 1734.7 KiB\n",
      "25/06/08 21:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1735.7 KiB\n",
      "25/06/08 21:59:15 WARN DAGScheduler: Broadcasting large task binary with size 1736.9 KiB\n",
      "25/06/08 21:59:19 WARN DAGScheduler: Broadcasting large task binary with size 1739.6 KiB\n",
      "25/06/08 21:59:24 WARN DAGScheduler: Broadcasting large task binary with size 1744.0 KiB\n",
      "25/06/08 21:59:31 WARN DAGScheduler: Broadcasting large task binary with size 1749.6 KiB\n",
      "25/06/08 21:59:35 WARN DAGScheduler: Broadcasting large task binary with size 1750.7 KiB\n",
      "25/06/08 21:59:39 WARN DAGScheduler: Broadcasting large task binary with size 1751.2 KiB\n",
      "25/06/08 21:59:43 WARN DAGScheduler: Broadcasting large task binary with size 1753.9 KiB\n",
      "25/06/08 21:59:47 WARN DAGScheduler: Broadcasting large task binary with size 1756.9 KiB\n",
      "25/06/08 21:59:53 WARN DAGScheduler: Broadcasting large task binary with size 1763.0 KiB\n",
      "25/06/08 22:00:02 WARN DAGScheduler: Broadcasting large task binary with size 1767.3 KiB\n",
      "25/06/08 22:00:06 WARN DAGScheduler: Broadcasting large task binary with size 1767.7 KiB\n",
      "25/06/08 22:00:10 WARN DAGScheduler: Broadcasting large task binary with size 1768.8 KiB\n",
      "25/06/08 22:00:14 WARN DAGScheduler: Broadcasting large task binary with size 1770.0 KiB\n",
      "25/06/08 22:00:18 WARN DAGScheduler: Broadcasting large task binary with size 1774.3 KiB\n",
      "25/06/08 22:00:24 WARN DAGScheduler: Broadcasting large task binary with size 1779.9 KiB\n",
      "25/06/08 22:00:33 WARN DAGScheduler: Broadcasting large task binary with size 1784.4 KiB\n",
      "25/06/08 22:00:36 WARN DAGScheduler: Broadcasting large task binary with size 1784.8 KiB\n",
      "25/06/08 22:00:40 WARN DAGScheduler: Broadcasting large task binary with size 1785.4 KiB\n",
      "25/06/08 22:00:44 WARN DAGScheduler: Broadcasting large task binary with size 1788.9 KiB\n",
      "25/06/08 22:00:48 WARN DAGScheduler: Broadcasting large task binary with size 1792.3 KiB\n",
      "25/06/08 22:00:54 WARN DAGScheduler: Broadcasting large task binary with size 1797.8 KiB\n",
      "25/06/08 22:01:03 WARN DAGScheduler: Broadcasting large task binary with size 1801.8 KiB\n",
      "25/06/08 22:01:07 WARN DAGScheduler: Broadcasting large task binary with size 1802.3 KiB\n",
      "25/06/08 22:01:11 WARN DAGScheduler: Broadcasting large task binary with size 1802.8 KiB\n",
      "25/06/08 22:01:15 WARN DAGScheduler: Broadcasting large task binary with size 1804.8 KiB\n",
      "25/06/08 22:01:19 WARN DAGScheduler: Broadcasting large task binary with size 1808.7 KiB\n",
      "25/06/08 22:01:25 WARN DAGScheduler: Broadcasting large task binary with size 1814.2 KiB\n",
      "25/06/08 22:01:33 WARN DAGScheduler: Broadcasting large task binary with size 1818.7 KiB\n",
      "25/06/08 22:01:37 WARN DAGScheduler: Broadcasting large task binary with size 1819.7 KiB\n",
      "25/06/08 22:01:40 WARN DAGScheduler: Broadcasting large task binary with size 1820.2 KiB\n",
      "25/06/08 22:01:44 WARN DAGScheduler: Broadcasting large task binary with size 1822.9 KiB\n",
      "25/06/08 22:01:49 WARN DAGScheduler: Broadcasting large task binary with size 1825.9 KiB\n",
      "25/06/08 22:01:55 WARN DAGScheduler: Broadcasting large task binary with size 1833.1 KiB\n",
      "25/06/08 22:02:04 WARN DAGScheduler: Broadcasting large task binary with size 1836.3 KiB\n",
      "25/06/08 22:02:07 WARN DAGScheduler: Broadcasting large task binary with size 1836.8 KiB\n",
      "25/06/08 22:02:11 WARN DAGScheduler: Broadcasting large task binary with size 1837.8 KiB\n",
      "25/06/08 22:02:15 WARN DAGScheduler: Broadcasting large task binary with size 1839.9 KiB\n",
      "25/06/08 22:02:19 WARN DAGScheduler: Broadcasting large task binary with size 1842.2 KiB\n",
      "25/06/08 22:02:25 WARN DAGScheduler: Broadcasting large task binary with size 1846.3 KiB\n",
      "25/06/08 22:02:32 WARN DAGScheduler: Broadcasting large task binary with size 1849.8 KiB\n",
      "25/06/08 22:02:36 WARN DAGScheduler: Broadcasting large task binary with size 1850.3 KiB\n",
      "25/06/08 22:02:40 WARN DAGScheduler: Broadcasting large task binary with size 1850.8 KiB\n",
      "25/06/08 22:02:44 WARN DAGScheduler: Broadcasting large task binary with size 1853.6 KiB\n",
      "25/06/08 22:02:48 WARN DAGScheduler: Broadcasting large task binary with size 1856.7 KiB\n",
      "25/06/08 22:02:55 WARN DAGScheduler: Broadcasting large task binary with size 1861.6 KiB\n",
      "25/06/08 22:03:03 WARN DAGScheduler: Broadcasting large task binary with size 1865.8 KiB\n",
      "25/06/08 22:03:07 WARN DAGScheduler: Broadcasting large task binary with size 1866.3 KiB\n",
      "25/06/08 22:03:11 WARN DAGScheduler: Broadcasting large task binary with size 1867.5 KiB\n",
      "25/06/08 22:03:15 WARN DAGScheduler: Broadcasting large task binary with size 1868.7 KiB\n",
      "25/06/08 22:03:20 WARN DAGScheduler: Broadcasting large task binary with size 1872.4 KiB\n",
      "25/06/08 22:03:26 WARN DAGScheduler: Broadcasting large task binary with size 1877.3 KiB\n",
      "25/06/08 22:03:34 WARN DAGScheduler: Broadcasting large task binary with size 1879.4 KiB\n",
      "25/06/08 22:03:38 WARN DAGScheduler: Broadcasting large task binary with size 1879.8 KiB\n",
      "25/06/08 22:03:41 WARN DAGScheduler: Broadcasting large task binary with size 1880.4 KiB\n",
      "25/06/08 22:03:45 WARN DAGScheduler: Broadcasting large task binary with size 1883.4 KiB\n",
      "25/06/08 22:03:50 WARN DAGScheduler: Broadcasting large task binary with size 1885.7 KiB\n",
      "25/06/08 22:03:56 WARN DAGScheduler: Broadcasting large task binary with size 1890.6 KiB\n",
      "25/06/08 22:04:05 WARN DAGScheduler: Broadcasting large task binary with size 1893.3 KiB\n",
      "25/06/08 22:04:09 WARN DAGScheduler: Broadcasting large task binary with size 1894.3 KiB\n",
      "25/06/08 22:04:13 WARN DAGScheduler: Broadcasting large task binary with size 1894.9 KiB\n",
      "25/06/08 22:04:17 WARN DAGScheduler: Broadcasting large task binary with size 1897.6 KiB\n",
      "25/06/08 22:04:21 WARN DAGScheduler: Broadcasting large task binary with size 1901.3 KiB\n",
      "25/06/08 22:04:28 WARN DAGScheduler: Broadcasting large task binary with size 1907.8 KiB\n",
      "25/06/08 22:04:35 WARN DAGScheduler: Broadcasting large task binary with size 1912.0 KiB\n",
      "25/06/08 22:04:39 WARN DAGScheduler: Broadcasting large task binary with size 1912.5 KiB\n",
      "25/06/08 22:04:43 WARN DAGScheduler: Broadcasting large task binary with size 1913.1 KiB\n",
      "25/06/08 22:04:47 WARN DAGScheduler: Broadcasting large task binary with size 1915.3 KiB\n",
      "25/06/08 22:04:52 WARN DAGScheduler: Broadcasting large task binary with size 1918.7 KiB\n",
      "25/06/08 22:04:57 WARN DAGScheduler: Broadcasting large task binary with size 1923.3 KiB\n",
      "25/06/08 22:05:05 WARN DAGScheduler: Broadcasting large task binary with size 1927.4 KiB\n",
      "25/06/08 22:05:09 WARN DAGScheduler: Broadcasting large task binary with size 1927.9 KiB\n",
      "25/06/08 22:05:12 WARN DAGScheduler: Broadcasting large task binary with size 1928.5 KiB\n",
      "25/06/08 22:05:16 WARN DAGScheduler: Broadcasting large task binary with size 1930.5 KiB\n",
      "25/06/08 22:05:21 WARN DAGScheduler: Broadcasting large task binary with size 1932.5 KiB\n",
      "25/06/08 22:05:26 WARN DAGScheduler: Broadcasting large task binary with size 1937.0 KiB\n",
      "25/06/08 22:05:34 WARN DAGScheduler: Broadcasting large task binary with size 1940.2 KiB\n",
      "25/06/08 22:05:38 WARN DAGScheduler: Broadcasting large task binary with size 1941.2 KiB\n",
      "25/06/08 22:05:42 WARN DAGScheduler: Broadcasting large task binary with size 1941.8 KiB\n",
      "25/06/08 22:05:46 WARN DAGScheduler: Broadcasting large task binary with size 1943.0 KiB\n",
      "25/06/08 22:05:50 WARN DAGScheduler: Broadcasting large task binary with size 1948.4 KiB\n",
      "25/06/08 22:05:57 WARN DAGScheduler: Broadcasting large task binary with size 1955.3 KiB\n",
      "25/06/08 22:06:05 WARN DAGScheduler: Broadcasting large task binary with size 1958.8 KiB\n",
      "25/06/08 22:06:09 WARN DAGScheduler: Broadcasting large task binary with size 1959.3 KiB\n",
      "25/06/08 22:06:13 WARN DAGScheduler: Broadcasting large task binary with size 1960.5 KiB\n",
      "25/06/08 22:06:17 WARN DAGScheduler: Broadcasting large task binary with size 1961.7 KiB\n",
      "25/06/08 22:06:22 WARN DAGScheduler: Broadcasting large task binary with size 1964.5 KiB\n",
      "25/06/08 22:06:28 WARN DAGScheduler: Broadcasting large task binary with size 1969.4 KiB\n",
      "25/06/08 22:06:36 WARN DAGScheduler: Broadcasting large task binary with size 1972.5 KiB\n",
      "25/06/08 22:06:40 WARN DAGScheduler: Broadcasting large task binary with size 1973.0 KiB\n",
      "25/06/08 22:06:44 WARN DAGScheduler: Broadcasting large task binary with size 1974.2 KiB\n",
      "25/06/08 22:06:48 WARN DAGScheduler: Broadcasting large task binary with size 1975.4 KiB\n",
      "25/06/08 22:06:52 WARN DAGScheduler: Broadcasting large task binary with size 1978.1 KiB\n",
      "25/06/08 22:06:59 WARN DAGScheduler: Broadcasting large task binary with size 1983.8 KiB\n",
      "25/06/08 22:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1986.2 KiB\n",
      "25/06/08 22:07:11 WARN DAGScheduler: Broadcasting large task binary with size 1986.7 KiB\n",
      "25/06/08 22:07:15 WARN DAGScheduler: Broadcasting large task binary with size 1987.9 KiB\n",
      "25/06/08 22:07:19 WARN DAGScheduler: Broadcasting large task binary with size 1989.1 KiB\n",
      "25/06/08 22:07:24 WARN DAGScheduler: Broadcasting large task binary with size 1991.5 KiB\n",
      "25/06/08 22:07:30 WARN DAGScheduler: Broadcasting large task binary with size 1995.8 KiB\n",
      "25/06/08 22:07:39 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "25/06/08 22:08:50 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ GBT – AUC-ROC: 0.8755 | PR-AUC: 0.5256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 22:10:00 WARN DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n",
      "25/06/08 22:10:42 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/06/08 22:11:38 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/06/08 22:12:33 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/06/08 22:13:13 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/06/08 22:13:13 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "25/06/08 22:14:02 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/06/08 22:15:24 WARN DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n",
      "25/06/08 22:15:24 ERROR Executor: Exception in task 3.0 in stage 3391.0 (TID 1973510)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:24 ERROR Executor: Exception in task 1.0 in stage 3391.0 (TID 1973508)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:24 ERROR Executor: Exception in task 9.0 in stage 3391.0 (TID 1973516)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:24 ERROR Executor: Exception in task 6.0 in stage 3391.0 (TID 1973513)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:24 ERROR Executor: Exception in task 8.0 in stage 3391.0 (TID 1973515)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:24 WARN TaskSetManager: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/06/08 22:15:24 ERROR TaskSetManager: Task 9 in stage 3391.0 failed 1 times; aborting job\n",
      "25/06/08 22:15:24 ERROR Executor: Exception in task 13.0 in stage 3391.0 (TID 1973520)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:25 ERROR Executor: Exception in task 0.0 in stage 3391.0 (TID 1973507)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:25 ERROR Executor: Exception in task 12.0 in stage 3391.0 (TID 1973519)\n",
      "breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 22:15:25 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.trainImpl(GaussianMixture.scala:457)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$fit$1(GaussianMixture.scala:409)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:382)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:330)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 7.0 in stage 3391.0 (TID 1973514) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 10.0 in stage 3391.0 (TID 1973517) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 5.0 in stage 3391.0 (TID 1973512) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 4.0 in stage 3391.0 (TID 1973511) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 11.0 in stage 3391.0 (TID 1973518) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 2.0 in stage 3391.0 (TID 1973509) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 17.0 in stage 3391.0 (TID 1973524) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 15.0 in stage 3391.0 (TID 1973522) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 18.0 in stage 3391.0 (TID 1973525) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 14.0 in stage 3391.0 (TID 1973521) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/08 22:15:25 WARN TaskSetManager: Lost task 16.0 in stage 3391.0 (TID 1973523) (192.168.100.73 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n",
      "\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n",
      "\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n",
      "\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n",
      "\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n",
      "\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n",
      "\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n",
      "\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n",
      "\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12426.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.ml.clustering.GaussianMixture.trainImpl(GaussianMixture.scala:457)\n\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$fit$1(GaussianMixture.scala:409)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n\tat org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:382)\n\tat org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:330)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     88\u001b[0m pipeline_gmm \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[assembler_num, scaler, gmm])\n\u001b[0;32m---> 89\u001b[0m gmm_model    \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_gmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m gmm_result \u001b[38;5;241m=\u001b[39m gmm_model\u001b[38;5;241m.\u001b[39mtransform(M_df)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     92\u001b[0m silhouette  \u001b[38;5;241m=\u001b[39m ClusteringEvaluator(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     93\u001b[0m                                   predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m                                   metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilhouette\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mevaluate(gmm_result)\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/ml/pipeline.py:138\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    136\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/ml/util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/ml/wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/ml/wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/texto-clean/lib/python3.9/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o12426.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 3391.0 failed 1 times, most recent failure: Lost task 9.0 in stage 3391.0 (TID 1973516) (192.168.100.73 executor driver): breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.ml.clustering.GaussianMixture.trainImpl(GaussianMixture.scala:457)\n\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$fit$1(GaussianMixture.scala:409)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n\tat org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:382)\n\tat org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:330)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: breeze.linalg.MatrixNotSymmetricException: Matrix is not symmetric\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$2(package.scala:152)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.$anonfun$requireSymmetricMatrix$1(package.scala:150)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n\tat breeze.linalg.package$.requireSymmetricMatrix(package.scala:150)\n\tat breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:140)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:113)\n\tat breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:111)\n\tat breeze.generic.UFunc.apply(UFunc.scala:47)\n\tat breeze.generic.UFunc.apply$(UFunc.scala:46)\n\tat breeze.linalg.eigSym$.apply(eig.scala:108)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:113)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple$lzycompute(MultivariateGaussian.scala:57)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.tuple(MultivariateGaussian.scala:56)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu$lzycompute(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.rootSigmaInvMulMu(MultivariateGaussian.scala:64)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.logpdf(MultivariateGaussian.scala:79)\n\tat org.apache.spark.ml.stat.distribution.MultivariateGaussian.pdf(MultivariateGaussian.scala:71)\n\tat org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:672)\n\tat org.apache.spark.ml.clustering.GaussianMixture.$anonfun$trainImpl$1(GaussianMixture.scala:448)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:866)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:866)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# ------------------ 4. Entrenamiento optimizado ------------------\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# ≥ 200 tareas shuffle para paralelismo\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "# 🔄 Asegurar que M_df existe (unión completa de la muestra)\n",
    "M_df = M_df if \"M_df\" in globals() else train_df.unionByName(test_df).cache()\n",
    "\n",
    "# 🔍  Relleno definitivo anti-NaN en numéricas\n",
    "def fill_numerics(df):\n",
    "    num_cols = [c for c, t in df.dtypes if t in (\"double\",\"float\")]\n",
    "    return df.fillna(-999, subset=num_cols)\n",
    "\n",
    "train_df  = fill_numerics(train_df)\n",
    "test_df   = fill_numerics(test_df)\n",
    "M_df      = fill_numerics(M_df)\n",
    "\n",
    "# 1️⃣ Columnas\n",
    "numeric_cols = [c for c, t in train_df.dtypes\n",
    "                if t in (\"double\",\"float\",\"int\",\"bigint\",\"long\") and c != \"isFraud\"]\n",
    "categorical_cols = [c for c in train_df.columns\n",
    "                    if c not in numeric_cols + [\"isFraud\",\"TransactionID\"]]\n",
    "\n",
    "MAX_CAT = 100  # umbral de cardinalidad\n",
    "small_cats = [c for c in categorical_cols\n",
    "              if train_df.select(c).distinct().count() <= MAX_CAT]\n",
    "large_cats = list(set(categorical_cols) - set(small_cats))\n",
    "print(f\"Indexaremos {len(small_cats)} columnas categóricas; \"\n",
    "      f\"omitimos {len(large_cats)} muy grandes.\")\n",
    "\n",
    "# 2️⃣ Indexar categóricas\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\",\n",
    "                          handleInvalid=\"keep\")\n",
    "            for c in small_cats]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols + [f\"{c}_idx\" for c in small_cats],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\")\n",
    "\n",
    "# 4️⃣ GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"isFraud\",\n",
    "                    featuresCol=\"features\",\n",
    "                    maxIter=25,\n",
    "                    seed=42)\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=indexers + [assembler, gbt])\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [4, 6])\n",
    "             .addGrid(gbt.maxBins,  [128])      # seguro ≤ MAX_CAT\n",
    "             .build())\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline_gbt,\n",
    "                           evaluator=BinaryClassificationEvaluator(\n",
    "                               labelCol=\"isFraud\", metricName=\"areaUnderROC\"),\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           trainRatio=0.8,\n",
    "                           parallelism=4,\n",
    "                           seed=42)\n",
    "\n",
    "best_model = tvs.fit(train_df).bestModel\n",
    "pred_gbt   = best_model.transform(test_df)\n",
    "\n",
    "evaluator_roc = BinaryClassificationEvaluator(labelCol=\"isFraud\",\n",
    "                                              metricName=\"areaUnderROC\")\n",
    "evaluator_pr  = BinaryClassificationEvaluator(labelCol=\"isFraud\",\n",
    "                                              metricName=\"areaUnderPR\")\n",
    "\n",
    "print(f\"\\n✅ GBT – AUC-ROC: {evaluator_roc.evaluate(pred_gbt):.4f} | \"\n",
    "      f\"PR-AUC: {evaluator_pr.evaluate(pred_gbt):.4f}\")\n",
    "\n",
    "# 5️⃣ Gaussian Mixture\n",
    "assembler_num = VectorAssembler(inputCols=numeric_cols,\n",
    "                                outputCol=\"num_features\")\n",
    "scaler = StandardScaler(inputCol=\"num_features\",\n",
    "                        outputCol=\"scaled_features\",\n",
    "                        withMean=True, withStd=True)\n",
    "gmm = GaussianMixture(featuresCol=\"scaled_features\", k=3, seed=42)\n",
    "\n",
    "pipeline_gmm = Pipeline(stages=[assembler_num, scaler, gmm])\n",
    "gmm_model    = pipeline_gmm.fit(M_df)\n",
    "\n",
    "gmm_result = gmm_model.transform(M_df).cache()\n",
    "silhouette  = ClusteringEvaluator(featuresCol=\"scaled_features\",\n",
    "                                  predictionCol=\"prediction\",\n",
    "                                  metricName=\"silhouette\").evaluate(gmm_result)\n",
    "print(f\"✅ GMM – Silhouette: {silhouette:.4f}\")\n",
    "\n",
    "print(\"\\nFraude por componente GMM:\")\n",
    "gmm_result.groupBy(\"prediction\", \"isFraud\").count()\\\n",
    "          .orderBy(\"prediction\", \"isFraud\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515eb17b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texto-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
